{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discourse Atom Topic Modeling (DATM) Tutorial \n",
    "\n",
    "## Part 2 of 2: Mapping Atoms to Text Data\n",
    "\n",
    "* This code is written in Python 3.7.2, and uses Gensim version 3.8.3. \n",
    "\n",
    "* This code is provides an the outline of how we assigned atoms to our cleaned data, which we show how to identify in Part 1 of 2. Note that we cannot redistribute the data used in our paper \"Integrating Topic Modeling and Word Embedding\" in any form, and researchers must apply directly to the Centers for Disease Control and Prevention for access. Details on data access are provided in the paper. We add comments with tips for adapting this code to your data.  \n",
    "* In our case, the goal of this code is to take a given narrative, get rolling windows of contexts from this narrative, find the SIF sentence embedding from each rolling window, and match the SIF embedding onto the closest (by cosine similarity) atom in the Dictionary loaded in earler. The SIF embedding is the maximum a posteriori (MAP) estimate of what the atom is for that sentence. So we'll get out a rolling window (i.e., a sequence) of atoms underlying the narrative.\n",
    "* In our case, we get atoms separately for law enforcement (narle) and medical examiner (narcme) narratives, and then combine the two distributions, as described in our paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import cython\n",
    "import pickle\n",
    "from gensim.models import Word2Vec \n",
    "from sklearn.preprocessing import normalize\n",
    "from random import sample\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import math\n",
    "from scipy.linalg import norm\n",
    "from collections import Counter\n",
    "from ksvd import ApproximateKSVD \n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import re\n",
    "import string, re\n",
    "import numpy as np \n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from random import seed, sample\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in a word embedding model trained on the corpus (in our case, violent death narratives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodel=Word2Vec.load('')\n",
    "#w2vmodel.init_sims(replace=False) #normalize word-vector lengths. May speed up if set replace=True, but then can't go back to the original (non-normalized) word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in dictionary of atoms identified in your word embedding (see \"DATM Tutorial Part 1 of 2\" for code) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back in the pickled dictionary of atom vectors\n",
    "infile = open('','rb')\n",
    "dictionary=pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare two pieces of informatin from the text data, which we will need to compute SIF Sentence Embeddings (MAP) of any given sentence \n",
    "\n",
    "* SIF Sentence Embedding is from: \"A Simple but tough-to-beat baseline for sentence embedding\" https://github.com/PrincetonML/SIF\n",
    "* To do SIF embeddings, we need to prep functions and two pieces of information from the raw text: (1) frequency weights for each word, and (2) the \"common discourse vector\" ($C_0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"\", 'rb') \n",
    "#this is the text data which has already been turned into trigrams and bigrams, cleaned, and tokenized. It is a list of lists, where each record is a word tokenized \"document\"\n",
    "corpus= pickle.load(file) \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(corpus) #list of 307249 narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The first input to SIF embeddings is an estimate of the frequency weights (based on probabilites) for each word in the corpus. Compute this here.**\n",
    "* This will naturally downweight stopwords when we compute a sentence embedding. It requires the raw text data of the corpus. \n",
    "* Either train a dictonary of weights (1), or upload a saved dictionary (2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_freq_dict(w2vmodel, weight_a=.001): #reasonable range for weight a is .001 to .0001 based on Arora et al SIF embeddings. The extent to which we re-weight words is controlled by the parameter $a$, where a lower value for $a$ means that frequent words are more aggressively down-weighted compared to less frequent words. \n",
    "    freq_dictionary = {word: w2vmodel.wv.vocab[word].count for word in w2vmodel.wv.vocab} \n",
    "    total= sum(freq_dictionary.values())\n",
    "    freq_dictionary = {word: weight_a/(weight_a + (w2vmodel.wv.vocab[word].count / total)) for word in w2vmodel.wv.vocab} #best values according to arora et al are between .001 and .0001\n",
    "    return(freq_dictionary)\n",
    "\n",
    "#function to yield a weighted sentence, using the above weight dictionary\n",
    "def get_weighted_sent(tokedsent,freq_dict, w2vmodel=w2vmodel): \n",
    "    weightedsent= [freq_dict[word]*w2vmodel.wv[word] for word in tokedsent if word in freq_dict.keys()]\n",
    "    return(sum(weightedsent)/len(weightedsent)) #weightedsent_avg  #divide the weighted average by the number of words in the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dict= get_freq_dict(w2vmodel, weight_a=.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. The second input to SIF embeddings is $C_0$, the common discourse vector. Compute this here.**\n",
    "* Get this with a random sample of discourse vectors since the data is so large, or compute using all narratives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def samp_cts(docs, n_sample, windowsize, freq_dictionary):\n",
    "    sampnarrs=  sample(docs, n_sample) #sample of narratives. Will take 1 random window and discourse vector of this window, from each narrative. \n",
    "    sampvecs= []\n",
    "\n",
    "\n",
    "    t1_start = perf_counter()  \n",
    "\n",
    "    for i in sampnarrs: #adjusting here to corpus sample, but consider using full corpus for final SIF embeddings. \n",
    "        if len(i)>windowsize: #want window length to be at least windowsize words\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            sampvecs.append(get_weighted_sent(i, freq_dictionary)) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "            n= sample(range(0,len(i)-windowsize), 1)[0] #get some random positon in the narrative (at least windowsize steps behind the last one though)\n",
    "            sent= i[n:n+windowsize] #random context window \n",
    "            sampvecs.append(get_weighted_sent(i, freq_dictionary)) #sample a discourse vector, and append to a list of sample discourse vectors.\n",
    "    sampvecs= np.asarray(sampvecs)\n",
    "    t1_stop = perf_counter() #for 100k context windows takes  \n",
    "    print(t1_stop-t1_start)\n",
    "    return(sampvecs)\n",
    "\n",
    "def get_c0(sampvecs):\n",
    "    svd = TruncatedSVD(n_components=1, n_iter=10, random_state=0) #only keeping top component, using same method as in SIF embedding code\n",
    "    svd.fit(sampvecs) #1st singular vector  is now c_o\n",
    "    return(svd.components_[0])\n",
    "\n",
    "def remove_c0(comdiscvec, modcontextvecs):\n",
    "    curcontextvec= [X - X.dot(comdiscvec.transpose()) * comdiscvec for X in modcontextvecs] #remove c_0 from all the cts\n",
    "    curcontextvec=np.asarray(curcontextvec)\n",
    "    return(curcontextvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampvecs2_narcme= samp_cts(list(corpus['narcme_toked']), 50000, 10, freq_dict) #we used a random sample of 50,000 context vectors\n",
    "sampvecs2_narcme= normalize(sampvecs2_narcme, axis=1) #l2 normalize the resulting context vectors\n",
    "\n",
    "sampvecs2_narle= samp_cts(list(corpus['narle_toked']), 50000, 10, freq_dict) #we used random sample of 50,000 context vectors\n",
    "sampvecs2_narle= normalize(sampvecs2_narle, axis=1) #l2 normalize the resulting context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc0_narcme= get_c0(sampvecs2_narcme)\n",
    "pc0_narle= get_c0(sampvecs2_narle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampvecs2_narcme = remove_c0(pc0_narcme, sampvecs2_narcme) \n",
    "sampvecs2_narle = remove_c0(pc0_narle, sampvecs2_narle) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Resulting function to get SIF MAPs along rolling windows, for a given narrative. \n",
    "\n",
    "* This the function we use to find rolling windows and assign MAPs to them, for a given narrative.\n",
    "* Note that this is set for our embedding size, which was 200-dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sif_atom_seqs(toked_narrative, window_size, topics_dictionary, c0_vector, freq_dict, w2vmodel): \n",
    "    \n",
    "    toked_narr2 = [i for i in toked_narrative if i in w2vmodel.wv.vocab] #remove words not in vocab\n",
    "    if len(toked_narr2)> 19 :  #this is set so that only narratives with at least 19 tokens in the w2v model vocab are considered. \n",
    "        it = iter(toked_narr2) \n",
    "        win = [next(it) for cnt in range(0,window_size)] #first context window\n",
    "        MAPs= normalize(remove_c0( c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,200))) #doing the SIF map here. Hardcoding in the dimensionality of the space to speed this up.\n",
    "        for e in it: # Subsequent windows\n",
    "            win[:-1] = win[1:]\n",
    "            win[-1] = e\n",
    "            MAPs = np.vstack((MAPs, normalize(remove_c0(c0_vector, get_weighted_sent(win, freq_dict, w2vmodel).reshape(1,200)))))  #this will be matrix of MAPs\n",
    "\n",
    "        costri= linear_kernel(MAPs, topics_dictionary) \n",
    "        atomsseq= np.argmax(costri, axis=1) #this is for the index of the closest atom to each of the MAPs\n",
    "        #maxinRow = np.amax(costri, axis=1) #this is for the closest atom's cossim value to each of the maps\n",
    "        return(atomsseq.tolist()) #returns sequence of the closest atoms to the MAPs\n",
    "    else:\n",
    "        return(None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample usage of sif_atom_seqs, on a single narrative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sif_atom_seqs(corpus[0], 10, dictionary , pc0, w2vmodel) #get SIFS then, get atoms. Returns N closest atoms, to N rolling windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Documents in a Corpus into a Sequences of Atoms \n",
    "\n",
    "* This is the **final result** we want from all code above\n",
    "* First, get c0 from narcme narratives, and then get the atom sequence for the narcme narratives\n",
    "* Then, get c0 from narle narratives, and then get the atom sequence for the narle narratives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SIF Atom Seqs on NARCME narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.2712618999999\n"
     ]
    }
   ],
   "source": [
    "corpus['narcme_atom_seq']= corpus['narcme_toked'].apply(lambda x: sif_atom_seqs(x, 10, dictionary , pc0_narcme, freq_dict, w2vmodel) )\n",
    "\n",
    "#convert the atom seq to a string of atoms, since this format is needed for the vectorizer (and easier to work with later) in a CSV, too\n",
    "corpus['narcme_atom_seq'] = corpus['narcme_atom_seq'].apply(lambda x: ' '.join([str(elem) for elem in x])  if(np.all(pd.notnull(x)))  else x ) #https://thispointer.com/python-how-to-convert-a-list-to-string/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get SIF Atom Seq on NARLE narratives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.07621459999973\n"
     ]
    }
   ],
   "source": [
    "corpus['narle_atom_seq']= corpus['narle_toked'].apply(lambda x: sif_atom_seqs(x, 10, dictionary , pc0_narle, freq_dict, w2vmodel) )\n",
    "\n",
    "#convert the atom seq to a string of atoms, since this format is needed for the vectorizer (and easier to work with later) in a CSV, too\n",
    "corpus['narle_atom_seq'] = corpus['narle_atom_seq'].apply(lambda x: ' '.join([str(elem) for elem in x])  if(np.all(pd.notnull(x)))  else x ) #https://thispointer.com/python-how-to-convert-a-list-to-string/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformatting the Resulting Atom Sequences into Variables, by Vectorizing the Atoms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in empty narratives in the data with NAs to avoid errors, and combine the two sequences (i.e., narcme and narle sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['narcme_atom_seq'] = corpus['narcme_atom_seq'].fillna('')\n",
    "corpus['narle_atom_seq'] = corpus['narle_atom_seq'].fillna('')\n",
    "corpus['narcme_narle_atom_seq_combined'] = corpus[\"narcme_atom_seq\"].map(str) + ' ' + corpus[\"narle_atom_seq\"].map(str) #combine these, adding in ' ' in  middle, this works find even if there is no entry in one of the narle or narcme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform each sequence into a distribution over topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_transformer = TfidfVectorizer(analyzer = 'word', norm='l1', use_idf=False, token_pattern='\\S+') #need token pattern, otherwise splits using using 0-9 single digits too! #note that atoms that are part of all or no documents will not be transformed here, can reset this default, but I left as is for now since makes prediction easier (fewer features). #includes l1 normalization so that longer documents don't get more weight, l1 normalizes with abs value but all our values are pos anyways\n",
    "bow_transformer.fit(corpus['narcme_narle_atom_seq_combined'].dropna(inplace=False)) #corpus needs to be in format ['word word word'], NOT tokenized already\n",
    "\n",
    "vecked = bow_transformer.transform(corpus['narcme_narle_atom_seq_combined'].tolist()).toarray() #consider instead:  vecked = bow_transformer.transform(corpus['narcme_narle_atom_seq_combined'].dropna(inplace=True).tolist()).toarray() #this is the \"feature\" data, now in an array for sklearn models\n",
    "corpus = pd.concat([corpus,pd.DataFrame(vecked, columns = bow_transformer.get_feature_names())], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save CSV with final atom assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv(\"\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
